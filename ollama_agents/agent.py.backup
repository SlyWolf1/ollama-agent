"""
Core Agent class implementation with thinking modes, tool calling, and handoff capabilities
"""
from __future__ import annotations
import asyncio
import time
from typing import Any, Dict, List, Optional, Union, Callable, TYPE_CHECKING
from dataclasses import dataclass, field
import ollama
from .tools import ToolRegistry
from .thinking import ThinkingMode, ThinkingManager
from .tracing import get_tracer, TraceLevel
from .model_settings import ModelSettings, DEFAULT_SETTINGS
from .stats import get_stats_tracker, StatType, TokenUsage
from .context_manager import TruncationStrategy
from .caching import get_cache
from .retry import RetryConfig, with_retry, async_with_retry
from .memory import MemoryManager, get_memory_manager, MemoryStore, InMemoryStore

if TYPE_CHECKING:
    from .handoff import AgentHandoff
    from .caching import ResponseCache


@dataclass
class Agent:
    """
    Advanced Ollama Agent with thinking modes, tool calling, and handoff capabilities.
    Follows OpenAI Agents SDK and Ollama naming conventions.
    """
    name: str
    instructions: Optional[str] = None
    model: Optional[str] = 'qwen3-vl:2b-thinking-q8_0'
    tools: List[Callable] = field(default_factory=list)
    handoffs: List['Agent'] = field(default_factory=list)
    
    # Common parameters (matching OpenAI/Ollama conventions)
    temperature: Optional[float] = None
    top_p: Optional[float] = None
    max_tokens: Optional[int] = None
    thinking_mode: Optional[ThinkingMode] = None
    
    # Ollama-specific parameters
    host: Optional[str] = None
    stream: bool = False
    keep_alive: Union[float, str, None] = None
    timeout: int = 30
    options: Optional[Dict[str, Any]] = None  # Advanced Ollama options
    
    # Deprecated: Use direct parameters instead
    settings: Optional[ModelSettings] = None
    
    # Tracing and monitoring (disabled by default for performance)
    trace_level: TraceLevel = TraceLevel.OFF
    enable_tracing: bool = False
    
    # Advanced features
    max_context_length: int = 20000
    context_truncation_strategy: TruncationStrategy = TruncationStrategy.SUMMARIZE_MIDDLE
    enable_cache: bool = False
    cache: Optional[Any] = None
    enable_retry: bool = False
    retry_config: Optional[Any] = None

    # Memory features
    enable_memory: bool = False
    memory_store: Optional[MemoryStore] = None

    # Initialized in __post_init__
    client: ollama.Client = field(init=False, repr=False)
    async_client: ollama.AsyncClient = field(init=False, repr=False)
    tool_registry: ToolRegistry = field(init=False, repr=False)
    thinking_manager: ThinkingManager = field(init=False, repr=False)
    tracer: Any = field(init=False, repr=False)
    stats_tracker: Any = field(init=False, repr=False)
    token_usage: TokenUsage = field(init=False, repr=False)
    messages: List[Dict[str, str]] = field(init=False, repr=False)
    handoff_manager: Optional[AgentHandoff] = field(init=False, default=None, repr=False)
    summary_threshold: int = field(init=False, repr=False)
    memory_manager: MemoryManager = field(init=False, repr=False)

    def __post_init__(self):
        from .handoff import AgentHandoff

        # Merge direct parameters with settings (direct params take precedence)
        # Build effective settings from direct parameters or legacy settings
        if self.settings is None:
            # Create settings from direct parameters
            self.settings = ModelSettings(
                temperature=self.temperature,
                top_p=self.top_p,
                max_tokens=self.max_tokens,
                thinking_mode=self.thinking_mode
            )
        else:
            # Merge: direct parameters override settings
            if self.temperature is not None:
                self.settings.temperature = self.temperature
            if self.top_p is not None:
                self.settings.top_p = self.top_p
            if self.max_tokens is not None:
                self.settings.max_tokens = self.max_tokens
            if self.thinking_mode is not None:
                self.settings.thinking_mode = self.thinking_mode

        # If no settings at all, use defaults
        if self.settings is None:
            self.settings = DEFAULT_SETTINGS

        # Initialize clients
        self.client = ollama.Client(host=self.host, timeout=self.timeout)
        self.async_client = ollama.AsyncClient(host=self.host, timeout=self.timeout)

        # Initialize managers
        self.tool_registry = ToolRegistry()
        self.thinking_manager = ThinkingManager()

        # Initialize tracing
        self.tracer = get_tracer()
        if self.enable_tracing:
            self.tracer.set_level(self.trace_level)

        # Initialize stats tracking
        self.stats_tracker = get_stats_tracker()
        self.token_usage = TokenUsage()

        # Register tools
        for tool_func in self.tools:
            self.tool_registry.register_tool(tool_func)

        # Initialize conversation history
        self.messages = []
        if self.instructions:
            self.messages.append({"role": "system", "content": self.instructions})

        # Initialize handoff manager
        if self.handoffs:
            agents_dict = {agent.name: agent for agent in self.handoffs}
            agents_dict[self.name] = self
            self.handoff_manager = AgentHandoff(agents_dict)
            self.handoff_manager.set_current_agent(self.name)

        # Initialize cache if enabled
        if self.enable_cache and self.cache is None:
            # Use global cache if available, otherwise create new one
            from .caching import ResponseCache, CacheStrategy
            global_cache = get_cache()
            if global_cache:
                self.cache = global_cache
            else:
                self.cache = ResponseCache(max_size=1000, strategy=CacheStrategy.LRU)

        # Initialize retry config if enabled
        if self.enable_retry and self.retry_config is None:
            self.retry_config = RetryConfig()

        # Initialize memory manager if enabled
        if self.enable_memory:
            if self.memory_store is not None:
                self.memory_manager = MemoryManager(self.memory_store)
            else:
                # Use default memory manager
                self.memory_manager = get_memory_manager()
        else:
            # Use in-memory store for basic functionality without persistent memory
            self.memory_manager = MemoryManager(InMemoryStore())

        # Context management settings
        self.summary_threshold = int(self.max_context_length * 0.75)  # When to trigger summarization (75% of max)

    def should_summarize_context(self) -> bool:
        """Check if the current context should be summarized"""
        current_length = sum(len(msg.get("content", "")) for msg in self.messages)
        return current_length > self.summary_threshold

    def summarize_context(self) -> str:
        """Summarize the current conversation context"""
        if not self.messages:
            return ""

        # Create a summary prompt for the LLM to summarize the conversation
        conversation_text = "\n".join([
            f"{msg['role']}: {msg['content']}"
            for msg in self.messages
            if msg['role'] in ['user', 'assistant']
        ])

        if len(conversation_text) < 100:  # If conversation is too short, no need to summarize
            return conversation_text

        # Use the model to generate a summary
        try:
            summary_prompt = f"Please provide a concise summary of the following conversation:\n\n{conversation_text}\n\nSummary:"
            summary_response = self.client.generate(
                model=self.model,
                prompt=summary_prompt,
                options={"num_predict": 200, "temperature": 0.3}
            )
            return summary_response.response
        except Exception:
            # If summarization fails, return a simple truncation
            return conversation_text[:1000] + "... [truncated]"

    def get_context_summary_message(self) -> Dict[str, str]:
        """Get a message containing the context summary"""
        summary = self.summarize_context()
        return {
            "role": "system",
            "content": f"Conversation summary: {summary}"
        }

    def add_tool(self, func: Callable):
        """Add a tool to the agent's tool registry"""
        self.tool_registry.register_tool(func)

    def set_thinking_mode(self, mode: ThinkingMode):
        """Set the agent's thinking mode"""
        self.settings.thinking_mode = mode

    def add_message(self, role: str, content: str):
        """Add a message to the conversation history"""
        self.messages.append({"role": role, "content": content})

    def _prepare_tools(self):
        """Prepare tools for Ollama API call"""
        return self.tool_registry.get_ollama_tools()

    def _get_think_param(self):
        """Get the appropriate think parameter based on thinking mode"""
        thinking_mode = self.settings.thinking_mode
        # Only return think parameter if explicitly set
        if thinking_mode is None:
            return None
        elif thinking_mode == ThinkingMode.NONE:
            return None
        elif thinking_mode == ThinkingMode.LOW:
            return "low"
        elif thinking_mode == ThinkingMode.MEDIUM:
            return "medium"
        elif thinking_mode == ThinkingMode.HIGH:
            return "high"
        else:
            return None

    def _get_options(self):
        """Get Ollama options based on configuration and thinking mode"""
        options = self.settings.to_ollama_options()
        thinking_mode = self.settings.thinking_mode or ThinkingMode.MEDIUM
        final_options = self.thinking_manager.apply_mode_to_options(thinking_mode, options)
        return final_options

    def chat(self, message: str, tools: Optional[List[Callable]] = None) -> Dict[str, Any]:
        """
        Send a message to the agent and get a response.
        Supports caching and retry.
        """
        if self.handoff_manager:
            # If a handoff manager exists, check for handoff rules
            target_agent_id = self.handoff_manager.check_handoff_rules(message)
            if target_agent_id and target_agent_id != self.name:
                return self.handoff_manager.handoff_to(target_agent_id, context={"trigger_message": message})

        start_time = time.time()

        with self.tracer.span("agent.chat", agent_id=self.name,
                             data={"message": message, "has_tools": bool(tools)}) as span:
            self.add_message("user", message)

            all_tools = self._prepare_tools()
            if tools:
                temp_registry = ToolRegistry()
                for tool_func in tools:
                    temp_registry.register_tool(tool_func)
                all_tools.extend(temp_registry.get_ollama_tools())

            self.stats_tracker.increment(StatType.TOOLS_CALLED, len(all_tools), agent_id=self.name)

            think_param = self._get_think_param()
            options = self._get_options()

            # # Check cache if enabled
            # if self.enable_cache and self.cache:
            #     cached_response = self.cache.get(
            #         message=message,
            #         model=self.model,
            #         options=options,
            #         tools=all_tools
            #     )
            #     if cached_response:
            #         self.stats_tracker.increment(StatType.CACHE_HITS, 1, agent_id=self.name)
            #         return cached_response

            self.stats_tracker.increment(StatType.REQUESTS_MADE, 1, agent_id=self.name)

            # # Make API call with retry if enabled
            # if self.enable_retry and self.retry_config:
            #     def on_retry_callback(exception, attempt):
            #         self.tracer.log(f"Retry attempt {attempt} after error: {exception}", agent_id=self.name)
                
            #     @with_retry(self.retry_config, on_retry=on_retry_callback)
            #     def make_request():
            #         return self.client.chat(
            #             model=self.model,
            #             messages=self.messages,
            #             tools=all_tools if all_tools else None,
            #             options=options,
            #             keep_alive=self.keep_alive,
            #             stream=self.stream,
            #             think=think_param
            #         )
                
            #     response = make_request()
            # else:
            # Build chat parameters
            chat_params = {
                'model': self.model,
                'messages': self.messages,
                'tools': all_tools if all_tools else None,
                'stream': self.stream,
                'options': options if options else None,
                'keep_alive': self.keep_alive
            }
            
            # Only include think parameter if explicitly set
            if think_param is not None:
                chat_params['think'] = think_param
            
            response = self.client.chat(**chat_params)

            response_time = time.time() - start_time
            self.stats_tracker.increment(StatType.RESPONSE_TIME, response_time, agent_id=self.name)

            if hasattr(response, 'prompt_eval_count') and response.prompt_eval_count:
                self.token_usage.add_usage(prompt_tokens=response.prompt_eval_count)
                self.stats_tracker.increment(StatType.TOKENS_INPUT, response.prompt_eval_count, agent_id=self.name)

            if hasattr(response, 'eval_count') and response.eval_count:
                self.token_usage.add_usage(completion_tokens=response.eval_count)
                self.stats_tracker.increment(StatType.TOKENS_OUTPUT, response.eval_count, agent_id=self.name)

            if hasattr(response.message, 'tool_calls') and response.message.tool_calls:
                self.stats_tracker.increment(StatType.TOOLS_SUCCESS, len(response.message.tool_calls), agent_id=self.name)

            self.add_message("assistant", response.message.content)
            self.stats_tracker.increment(StatType.CONVERSATION_TURNS, 1, agent_id=self.name)

            result = {
                "content": response.message.content,
                "tool_calls": getattr(response.message, 'tool_calls', None),
                "raw_response": response
            }
            
            # # Cache the response if caching is enabled
            # if self.enable_cache and self.cache:
            #     self.cache.set(
            #         message=message,
            #         model=self.model,
            #         response=result,
            #         options=options,
            #         tools=all_tools
            #     )
            
            return result

    async def achat(self, message: str, tools: Optional[List[Callable]] = None) -> Dict[str, Any]:
        """
        Asynchronously send a message to the agent and get a response
        """
        if self.handoff_manager:
            target_agent_id = self.handoff_manager.check_handoff_rules(message)
            if target_agent_id and target_agent_id != self.name:
                return self.handoff_manager.handoff_to(target_agent_id, context={"trigger_message": message})

        start_time = time.time()
        with self.tracer.span("agent.achat", agent_id=self.name,
                             data={"message": message, "has_tools": bool(tools)}) as span:
            self.add_message("user", message)
            all_tools = self._prepare_tools()
            if tools:
                temp_registry = ToolRegistry()
                for tool_func in tools:
                    temp_registry.register_tool(tool_func)
                all_tools.extend(temp_registry.get_ollama_tools())
            self.stats_tracker.increment(StatType.TOOLS_CALLED, len(all_tools), agent_id=self.name)
            think_param = self._get_think_param()
            options = self._get_options()
            self.stats_tracker.increment(StatType.REQUESTS_MADE, 1, agent_id=self.name)

            # Build chat parameters
            chat_params = {
                'model': self.model,
                'messages': self.messages,
                'tools': all_tools if all_tools else None,
                'stream': self.stream,
                'options': options if options else None,
                'keep_alive': self.keep_alive
            }
            
            # Only include think parameter if explicitly set
            if think_param is not None:
                chat_params['think'] = think_param
            
            response = await self.async_client.chat(**chat_params)

            response_time = time.time() - start_time
            self.stats_tracker.increment(StatType.RESPONSE_TIME, response_time, agent_id=self.name)
            if hasattr(response, 'prompt_eval_count') and response.prompt_eval_count:
                self.token_usage.add_usage(prompt_tokens=response.prompt_eval_count)
                self.stats_tracker.increment(StatType.TOKENS_INPUT, response.prompt_eval_count, agent_id=self.name)
            if hasattr(response, 'eval_count') and response.eval_count:
                self.token_usage.add_usage(completion_tokens=response.eval_count)
                self.stats_tracker.increment(StatType.TOKENS_OUTPUT, response.eval_count, agent_id=self.name)
            if hasattr(response.message, 'tool_calls') and response.message.tool_calls:
                self.stats_tracker.increment(StatType.TOOLS_SUCCESS, len(response.message.tool_calls), agent_id=self.name)
            self.add_message("assistant", response.message.content)
            self.stats_tracker.increment(StatType.CONVERSATION_TURNS, 1, agent_id=self.name)
            result = {
                "content": response.message.content,
                "tool_calls": getattr(response.message, 'tool_calls', None),
                "raw_response": response
            }
            return result
    
    def generate(self, prompt: str) -> Dict[str, Any]:
        """
        Generate content using the agent
        """
        start_time = time.time()
        with self.tracer.span("agent.generate", agent_id=self.name,
                             data={"prompt_length": len(prompt)}) as span:
            think_param = self._get_think_param()
            options = self._get_options()
            self.stats_tracker.increment(StatType.REQUESTS_MADE, 1, agent_id=self.name)
            
            # Build generate parameters
            gen_params = {
                'model': self.model,
                'prompt': prompt,
                'system': self.instructions,
                'stream': self.stream,
                'options': options if options else None,
                'keep_alive': self.keep_alive
            }
            
            # Only include think parameter if explicitly set
            if think_param is not None:
                gen_params['think'] = think_param
            
            response = self.client.generate(**gen_params)
            response_time = time.time() - start_time
            self.stats_tracker.increment(StatType.RESPONSE_TIME, response_time, agent_id=self.name)
            if hasattr(response, 'prompt_eval_count') and response.prompt_eval_count:
                self.token_usage.add_usage(prompt_tokens=response.prompt_eval_count)
                self.stats_tracker.increment(StatType.TOKENS_INPUT, response.prompt_eval_count, agent_id=self.name)
            if hasattr(response, 'eval_count') and response.eval_count:
                self.token_usage.add_usage(completion_tokens=response.eval_count)
                self.stats_tracker.increment(StatType.TOKENS_OUTPUT, response.eval_count, agent_id=self.name)
            result = {
                "content": response.response,
                "raw_response": response
            }
            return result
    
    async def agenerate(self, prompt: str) -> Dict[str, Any]:
        """
        Asynchronously generate content using the agent
        """
        with self.tracer.span("agent.agenerate", agent_id=self.name,
                             data={"prompt_length": len(prompt)}) as span:
            think_param = self._get_think_param()
            options = self._get_options()
            
            # Build generate parameters
            gen_params = {
                'model': self.model,
                'prompt': prompt,
                'system': self.instructions,
                'stream': self.stream,
                'options': options if options else None,
                'keep_alive': self.keep_alive
            }
            
            # Only include think parameter if explicitly set
            if think_param is not None:
                gen_params['think'] = think_param
            
            response = await self.async_client.generate(**gen_params)
            result = {
                "content": response.response,
                "raw_response": response
            }
            return result

    def reset_conversation(self):
        """Reset the conversation history"""
        self.messages = []
        if self.instructions:
            self.messages.append({"role": "system", "content": self.instructions})

    # Memory-related methods
    def remember(self, key: str, value: Any, expires_in: Optional[int] = None, metadata: Optional[Dict[str, Any]] = None) -> bool:
        """Store a memory entry for this agent"""
        return self.memory_manager.set(self.name, key, value, expires_in, metadata)

    def recall(self, key: str) -> Optional[Any]:
        """Retrieve a memory entry for this agent"""
        return self.memory_manager.get(self.name, key)

    def forget(self, key: str) -> bool:
        """Delete a memory entry for this agent"""
        return self.memory_manager.delete(self.name, key)

    def get_memory_keys(self) -> List[str]:
        """List all memory keys for this agent"""
        return self.memory_manager.list_keys(self.name)

    def clear_memory(self) -> bool:
        """Clear all memory for this agent"""
        return self.memory_manager.clear_agent_memory(self.name)

    def get_memory_metadata(self, key: str) -> Optional[Dict[str, Any]]:
        """Get metadata for a memory entry"""
        return self.memory_manager.get_metadata(self.name, key)

    def cleanup_expired_memory(self) -> int:
        """Clean up expired memory entries"""
        return self.memory_manager.cleanup_expired()

    def get_tracer(self):
        """Get the tracer instance for this agent"""
        return self.tracer

    def get_trace_store(self):
        """Get the trace store for this agent"""
        return self.tracer.get_trace_store()

    def export_trace_session(self, session_id: Optional[str] = None) -> str:
        """Export the trace session to JSON"""
        return self.tracer.export_session(session_id)

    async def __aenter__(self):
        """Async context manager entry"""
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        pass

    def __enter__(self):
        """Sync context manager entry"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Sync context manager exit"""
        pass